{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load bench.py\n",
    "\"\"\"\n",
    "A much shorter version of train.py for benchmarking\n",
    "\"\"\"\n",
    "import os\n",
    "from contextlib import nullcontext\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "from model import GPTConfig, GPT\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "batch_size = 12\n",
    "block_size = 1024\n",
    "bias = False\n",
    "real_data = True\n",
    "seed = 1337\n",
    "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n",
    "dtype = 'bfloat16' # 'float32' or 'bfloat16' or 'float16'\n",
    "compile = True # use PyTorch 2.0 to compile the model to be faster\n",
    "profile = False # use pytorch profiler, or just simple benchmarking?\n",
    "exec(open('configurator.py').read()) # overrides from command line or config file\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    "\n",
    "# data loading init\n",
    "if real_data:\n",
    "    dataset = 'openwebtext'\n",
    "    data_dir = os.path.join('data', dataset)\n",
    "    train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
    "    def get_batch(split):\n",
    "        data = train_data # note ignore split in benchmarking script\n",
    "        ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "        x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
    "        y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
    "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "        return x, y\n",
    "else:\n",
    "    # alternatively, if fixed data is desired to not care about data loading\n",
    "    x = torch.randint(50304, (batch_size, block_size), device=device)\n",
    "    y = torch.randint(50304, (batch_size, block_size), device=device)\n",
    "    get_batch = lambda split: (x, y)\n",
    "\n",
    "# model init\n",
    "gptconf = GPTConfig(\n",
    "    block_size = block_size, # how far back does the model look? i.e. context size\n",
    "    n_layer = 12, n_head = 12, n_embd = 768, # size of the model\n",
    "    dropout = 0, # for determinism\n",
    "    bias = bias,\n",
    ")\n",
    "model = GPT(gptconf)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = model.configure_optimizers(weight_decay=1e-2, learning_rate=1e-4, betas=(0.9, 0.95), device_type=device_type)\n",
    "\n",
    "if compile:\n",
    "    print(\"Compiling model...\")\n",
    "    model = torch.compile(model) # pytorch 2.0\n",
    "\n",
    "if profile:\n",
    "    # useful docs on pytorch profiler:\n",
    "    # - tutorial https://pytorch.org/tutorials/intermediate/tensorboard_profiler_tutorial.html\n",
    "    # - api https://pytorch.org/docs/stable/profiler.html#torch.profiler.profile\n",
    "    wait, warmup, active = 5, 5, 5\n",
    "    num_steps = wait + warmup + active\n",
    "    with torch.profiler.profile(\n",
    "        activities=[torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA],\n",
    "        schedule=torch.profiler.schedule(wait=wait, warmup=warmup, active=active, repeat=1),\n",
    "        on_trace_ready=torch.profiler.tensorboard_trace_handler('./bench_log'),\n",
    "        record_shapes=False,\n",
    "        profile_memory=False,\n",
    "        with_stack=False, # incurs an additional overhead, disable if not needed\n",
    "        with_flops=True,\n",
    "        with_modules=False, # only for torchscript models atm\n",
    "    ) as prof:\n",
    "\n",
    "        X, Y = get_batch('train')\n",
    "        for k in range(num_steps):\n",
    "            with ctx:\n",
    "                logits, loss = model(X, Y)\n",
    "            X, Y = get_batch('train')\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lossf = loss.item()\n",
    "            print(f\"{k}/{num_steps} loss: {lossf:.4f}\")\n",
    "\n",
    "            prof.step() # notify the profiler at end of each step\n",
    "\n",
    "else:\n",
    "\n",
    "    # simple benchmarking\n",
    "    torch.cuda.synchronize()\n",
    "    for stage, num_steps in enumerate([10, 20]): # burnin, then benchmark\n",
    "        t0 = time.time()\n",
    "        X, Y = get_batch('train')\n",
    "        for k in range(num_steps):\n",
    "            with ctx:\n",
    "                logits, loss = model(X, Y)\n",
    "            X, Y = get_batch('train')\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lossf = loss.item()\n",
    "            print(f\"{k}/{num_steps} loss: {lossf:.4f}\")\n",
    "        torch.cuda.synchronize()\n",
    "        t1 = time.time()\n",
    "        dt = t1-t0\n",
    "        mfu = model.estimate_mfu(batch_size * 1 * num_steps, dt)\n",
    "        if stage == 1:\n",
    "            print(f\"time per iteration: {dt/num_steps*1000:.4f}ms, MFU: {mfu*100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
